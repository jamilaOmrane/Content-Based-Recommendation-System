{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer,CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Products' data+\n",
    "item_data = gzip.open(\"/home/data/amazon-reviews/metadata.json.gz\",\"r\")\n",
    "\n",
    "def explore_data():   \n",
    "    for line in item_data:\n",
    "        yield json.dumps(eval(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing reviews in order to preserve only the items with more tahn 5 reviews\n",
    "review_file =  open(\"/home/wsaadallah/amazon-reviews/complete.json\",\"r\") \n",
    "review_lines_text = review_file.readlines(900000000)\n",
    "\n",
    "reviews=[]\n",
    "for rlt in review_lines_text:\n",
    "    reviews.append(json.loads(rlt))\n",
    "\n",
    "reviews_df = pd.DataFrame(reviews, columns=['asin','overall','reviewText'])\n",
    "\n",
    "count_reviews = reviews_df['asin'].value_counts()\n",
    "# Print(count_reviews)\n",
    "\n",
    "# Preserving only items with more tha 5 reviews\n",
    "items = []\n",
    "for line in explore_data():\n",
    "    json_item = json.loads(line)\n",
    "    if json_item[\"asin\"] in count_reviews and count_reviews[json_item[\"asin\"]] >= 5 :\n",
    "        items.append(json_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constructing items' dataframe\n",
    "items_df = pd.DataFrame(items, columns=[\"asin\",\"description\", \"title\", \"categories\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing items with nans\n",
    "filtred_items_df = items_df.dropna()\n",
    "# Reseting the index\n",
    "filtred_items_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the variables that we don't need anymore in order to empty the RAM\n",
    "items_df = None\n",
    "del items_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing items' dataframe\n",
    "filtred_items_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see, the categories field is a list of list of string. For simplification reasons, we will only maintain the first\n",
    "# So, we are extracting the first category and putting it in a new column 'category1'\n",
    "filtred_items_df['category1']= filtred_items_df['categories'].map(lambda x: [x[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying One Hot Encoder on the category1 field\n",
    "filtred_items_df = filtred_items_df.drop('category1', 1).join(filtred_items_df['category1'].str.join('|').str.get_dummies())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We think that setting dataFrame index to the product_id category which is the 'asin' field, will facilitate the access \n",
    "# to the item since it's a unique value\n",
    "filtred_items_df = filtred_items_df.set_index('asin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information Retrieval : TF-IDF vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building description Corpus:\n",
    "# Corpus is a dataset of textual data, our textual data is the description field\n",
    "descriptions_corpus = np.array(filtred_items_df['description']) \n",
    "\n",
    "# The preprocessing needed for TF-IDF is done in the next few steps:\n",
    "\n",
    "# To lowercase\n",
    "descriptions_corpus = [df.lower() for df in descriptions_corpus]\n",
    "\n",
    "# Remove numbers\n",
    "descriptions_corpus = [re.sub(r'\\d+', '', df) for df in descriptions_corpus]\n",
    " \n",
    "# Replace punctuation with space\n",
    "p = re.compile(\"[\" + re.escape(string.punctuation) + \"]\")\n",
    "descriptions_corpus = [p.sub(\" \", df) for df in descriptions_corpus]\n",
    "\n",
    "# Remove white space\n",
    "descriptions_corpus = [df.strip() for df in descriptions_corpus]\n",
    "\n",
    "# Tokenization\n",
    "tokenized_description_corpus = [nltk.word_tokenize(dc) for dc in descriptions_corpus]\n",
    "\n",
    "# Stemming\n",
    "stemmer= PorterStemmer()\n",
    "stemmed_description_corpus = [[stemmer.stem(word)for word in des]for des in tokenized_description_corpus]\n",
    "\n",
    "# The following function need to be passed in the 'TfidfVectorizer' funtion. since, our corpus is already tokenized, we create \n",
    "# a function that returns the same text\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "# Building the TF-IDF matrix\n",
    "tfidf = TfidfVectorizer(tokenizer=identity_tokenizer, stop_words='english', lowercase=False, max_features=100000, min_df=5)    \n",
    "tfidf_matrix = tfidf.fit_transform(stemmed_description_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe from this matrix: each row in this dataframe represent the featured vector of the desciption of an item \n",
    "tf_idf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to keep track of reach descrption, we concatenated the column 'asin' to the tf-idf dataframe\n",
    "tf_idf_df.insert(0,'myAsin',filtred_items_df.index)\n",
    "\n",
    "# And we set this field as an index:\n",
    "tf_idf_df.set_index('myAsin', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating the 2 matrixes of OHE and TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting by eliminating columns that we choose nt to use \n",
    "items_df = filtred_items_df.drop(columns=['description','title','categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation\n",
    "items_df = pd.concat([items_df,tf_idf_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Profiling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For memory sake, we got only the first 90000000 rows\n",
    "review_file =  open(\"/home/wsaadallah/amazon-reviews/complete.json\",\"r\") \n",
    "review_lines_text = review_file.readlines(90000000)\n",
    "\n",
    "#Getting reviews\n",
    "reviews=[]\n",
    "for rlt in review_lines_text:\n",
    "    json_rlt = json.loads(rlt)\n",
    "    if json_rlt['asin'] in items_df.index:\n",
    "        reviews.append(json_rlt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.DataFrame(reviews, columns=['asin', 'reviewerID', 'overall'], dtype=np.uint8)\n",
    "\n",
    "#Removing duplication\n",
    "reviews_df = reviews_df.drop_duplicates(['asin','reviewerID'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing useless variables\n",
    "del review_file, review_lines_text, reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building product x reviewer matrix (or dataframe), with the overall variable as values\n",
    "reviews_df = reviews_df.pivot(index='asin', columns='reviewerID', values='overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the nans in the obtained dataframe\n",
    "reviews_df= reviews_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping items_df to take only item that have reviews in the item x reviewer df\n",
    "indexes_to_drop = []\n",
    "for i in items_df.index:\n",
    "    if i not in reviews_df.index:\n",
    "        indexes_to_drop.append(i)\n",
    "items_df = items_df.drop(indexes_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the reviews_df by the item_df index\n",
    "reviews_df = reviews_df.reindex(items_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building reviewers' profiles\n",
    "reviewers_profiles = reviews_df.T.dot(items_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing reviewers' profiles\n",
    "\n",
    "# Constructing reviewers_ratings_count_df which calclate the number of reviews for each reviewer\n",
    "reviewers_ratings_count_df = pd.DataFrame(reviews_df.astype(bool).sum(axis=0))\n",
    "\n",
    "# Contactinating this df to the reviewers_profiles df\n",
    "reviewers_ratings_count_df = pd.concat([reviewers_profiles, reviewers_ratings_count_df], axis=1)\n",
    "\n",
    "# Normalizing this ds by the count column\n",
    "reviewer_normalized_profile = reviewers_ratings_count_df.apply(lambda row: row/row[0], axis=1)\n",
    "\n",
    "#Then remoming this count column\n",
    "reviewer_normalized_profile = reviewer_normalized_profile.drop(0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We used knn in order to determine the products to recommend:\n",
    "\n",
    "def neighbors(data):\n",
    "    return NearestNeighbors(n_neighbors=10, metric='cosine').fit(data)\n",
    "\n",
    "nbrs = neighbors(items_df)\n",
    "\n",
    "def predit_neigbors_for_user (user_index) :\n",
    "        distances, indices = nbrs.kneighbors(reviewer_normalized_profile.loc[user_index].values.reshape(1,-1))\n",
    "        #print(distances)\n",
    "        for i in indices:\n",
    "            index = items_df.iloc[i,:].index \n",
    "        return index.tolist()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting by passing the user_id. Here I mean reviewer by user.\n",
    "user_id = \"A00000262KYZUE4J55XGL\"\n",
    "user_predictions = predit_neigbors_for_user(user_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
